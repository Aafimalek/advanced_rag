{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be9c772",
   "metadata": {},
   "source": [
    "# Multimodal RAG over Complex Documents (PDF/DOCX/PPTX) — Gemini 2.5 Pro + Gemini Embeddings\n",
    "_Merged from your two notebooks; core logic preserved, models swapped to Gemini._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a56630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement langchain<2.0,>=1.0.6 (from versions: 0.0.1, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.9, 0.0.10, 0.0.11, 0.0.12, 0.0.13, 0.0.14, 0.0.15, 0.0.16, 0.0.17, 0.0.18, 0.0.19, 0.0.20, 0.0.21, 0.0.22, 0.0.23, 0.0.24, 0.0.25, 0.0.26, 0.0.27, 0.0.28, 0.0.29, 0.0.30, 0.0.31, 0.0.32, 0.0.33, 0.0.34, 0.0.35, 0.0.36, 0.0.37, 0.0.38, 0.0.39, 0.0.40, 0.0.41, 0.0.42, 0.0.43, 0.0.44, 0.0.45, 0.0.46, 0.0.47, 0.0.48, 0.0.49, 0.0.50, 0.0.51, 0.0.52, 0.0.53, 0.0.54, 0.0.55, 0.0.56, 0.0.57, 0.0.58, 0.0.59, 0.0.60, 0.0.61, 0.0.63, 0.0.64, 0.0.65, 0.0.66, 0.0.67, 0.0.68, 0.0.69, 0.0.70, 0.0.71, 0.0.72, 0.0.73, 0.0.74, 0.0.75, 0.0.76, 0.0.77, 0.0.78, 0.0.79, 0.0.80, 0.0.81, 0.0.82, 0.0.83, 0.0.84, 0.0.85, 0.0.86, 0.0.87, 0.0.88, 0.0.89, 0.0.90, 0.0.91, 0.0.92, 0.0.93, 0.0.94, 0.0.95, 0.0.96, 0.0.97, 0.0.98, 0.0.99rc0, 0.0.99, 0.0.100, 0.0.101rc0, 0.0.101, 0.0.102rc0, 0.0.102, 0.0.103, 0.0.104, 0.0.105, 0.0.106, 0.0.107, 0.0.108, 0.0.109, 0.0.110, 0.0.111, 0.0.112, 0.0.113, 0.0.114, 0.0.115, 0.0.116, 0.0.117, 0.0.118, 0.0.119, 0.0.120, 0.0.121, 0.0.122, 0.0.123, 0.0.124, 0.0.125, 0.0.126, 0.0.127, 0.0.128, 0.0.129, 0.0.130, 0.0.131, 0.0.132, 0.0.133, 0.0.134, 0.0.135, 0.0.136, 0.0.137, 0.0.138, 0.0.139, 0.0.140, 0.0.141, 0.0.142, 0.0.143, 0.0.144, 0.0.145, 0.0.146, 0.0.147, 0.0.148, 0.0.149, 0.0.150, 0.0.151, 0.0.152, 0.0.153, 0.0.154, 0.0.155, 0.0.156, 0.0.157, 0.0.158, 0.0.159, 0.0.160, 0.0.161, 0.0.162, 0.0.163, 0.0.164, 0.0.165, 0.0.166, 0.0.167, 0.0.168, 0.0.169, 0.0.170, 0.0.171, 0.0.172, 0.0.173, 0.0.174, 0.0.175, 0.0.176, 0.0.177, 0.0.178, 0.0.179, 0.0.180, 0.0.181, 0.0.182, 0.0.183, 0.0.184, 0.0.185, 0.0.186, 0.0.187, 0.0.188, 0.0.189, 0.0.190, 0.0.191, 0.0.192, 0.0.193, 0.0.194, 0.0.195, 0.0.196, 0.0.197, 0.0.198, 0.0.199, 0.0.200, 0.0.201, 0.0.202, 0.0.203, 0.0.204, 0.0.205, 0.0.206, 0.0.207, 0.0.208, 0.0.209, 0.0.210, 0.0.211, 0.0.212, 0.0.213, 0.0.214, 0.0.215, 0.0.216, 0.0.217, 0.0.218, 0.0.219, 0.0.220, 0.0.221, 0.0.222, 0.0.223, 0.0.224, 0.0.225, 0.0.226, 0.0.227, 0.0.228, 0.0.229, 0.0.230, 0.0.231, 0.0.232, 0.0.233, 0.0.234, 0.0.235, 0.0.236, 0.0.237, 0.0.238, 0.0.239, 0.0.240rc0, 0.0.240rc1, 0.0.240rc4, 0.0.240, 0.0.242, 0.0.243, 0.0.244, 0.0.245, 0.0.246, 0.0.247, 0.0.248, 0.0.249, 0.0.250, 0.0.251, 0.0.252, 0.0.253, 0.0.254, 0.0.255, 0.0.256, 0.0.257, 0.0.258, 0.0.259, 0.0.260, 0.0.261, 0.0.262, 0.0.263, 0.0.264, 0.0.265, 0.0.266, 0.0.267, 0.0.268, 0.0.269, 0.0.270, 0.0.271, 0.0.272, 0.0.273, 0.0.274, 0.0.275, 0.0.276, 0.0.277, 0.0.278, 0.0.279, 0.0.281, 0.0.283, 0.0.284, 0.0.285, 0.0.286, 0.0.287, 0.0.288, 0.0.289, 0.0.290, 0.0.291, 0.0.292, 0.0.293, 0.0.294, 0.0.295, 0.0.296, 0.0.297, 0.0.298, 0.0.299, 0.0.300, 0.0.301, 0.0.302, 0.0.303, 0.0.304, 0.0.305, 0.0.306, 0.0.307, 0.0.308, 0.0.309, 0.0.310, 0.0.311, 0.0.312, 0.0.313, 0.0.314, 0.0.315, 0.0.316, 0.0.317, 0.0.318, 0.0.319, 0.0.320, 0.0.321, 0.0.322, 0.0.323, 0.0.324, 0.0.325, 0.0.326, 0.0.327, 0.0.329, 0.0.330, 0.0.331rc0, 0.0.331rc1, 0.0.331rc2, 0.0.331rc3, 0.0.331, 0.0.332, 0.0.333, 0.0.334, 0.0.335, 0.0.336, 0.0.337, 0.0.338, 0.0.339rc0, 0.0.339rc1, 0.0.339rc2, 0.0.339rc3, 0.0.339, 0.0.340, 0.0.341, 0.0.342, 0.0.343, 0.0.344, 0.0.345, 0.0.346, 0.0.347, 0.0.348, 0.0.349rc1, 0.0.349rc2, 0.0.349, 0.0.350, 0.0.351, 0.0.352, 0.0.353, 0.0.354, 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.1.10, 0.1.11, 0.1.12, 0.1.13, 0.1.14, 0.1.15, 0.1.16, 0.1.17rc1, 0.1.17, 0.1.19, 0.1.20, 0.2.0rc1, 0.2.0rc2, 0.2.0, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.2.7, 0.2.8, 0.2.9, 0.2.10, 0.2.11, 0.2.12, 0.2.13, 0.2.14, 0.2.15, 0.2.16, 0.2.17, 0.3.0.dev1, 0.3.0.dev2, 0.3.0, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.3.5, 0.3.6, 0.3.7, 0.3.8, 0.3.9, 0.3.10, 0.3.11, 0.3.12, 0.3.13, 0.3.14, 0.3.15, 0.3.16, 0.3.17, 0.3.18rc1, 0.3.18rc2, 0.3.18, 0.3.19, 0.3.20, 0.3.21, 0.3.22, 0.3.23, 0.3.24, 0.3.25, 0.3.26, 0.3.27, 0.4.0.dev0, 1.0.0a1, 1.0.0a2, 1.0.0a3, 1.0.0a4, 1.0.0a5, 1.0.0a6, 1.0.0a7, 1.0.0a8, 1.0.0a9, 1.0.0a10, 1.0.0a11, 1.0.0a12, 1.0.0a13, 1.0.0a14, 1.0.0a15, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5)\n",
      "ERROR: No matching distribution found for langchain<2.0,>=1.0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "✔ Dependencies installed. If your kernel had older packages loaded, restart the kernel once.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Compatible, pinned deps for LangChain + Gemini + Chroma + Unstructured\n",
    "%pip install -Uq \"pip>=25.0\"\n",
    "\n",
    "# Core LangChain stack (v1.x line to avoid 'langchain.verbose' errors)\n",
    "%pip install -Uq \\\n",
    "  \"langchain>=1.0.6,<2.0\" \\\n",
    "  \"langchain-core>=1.0.6,<2.0\" \\\n",
    "  \"langchain-community>=0.4.1,<1.0\" \\\n",
    "  \"langchain-text-splitters>=1.0.0,<2.0\" \\\n",
    "  \"langchain-chroma>=1.0.0,<2.0\" \\\n",
    "  \"chromadb>=0.5.5,<0.6\"\n",
    "\n",
    "# Gemini integration (LangChain + official SDKs)\n",
    "%pip install -Uq \\\n",
    "  \"langchain-google-genai>=2.0.7,<3.0\" \\\n",
    "  \"google-generativeai>=0.7.2\" \\\n",
    "  \"google-genai>=0.2.0\" \\\n",
    "  \"python-dotenv>=1.0.1\" \\\n",
    "  \"pydantic>=2.7,<3\" \\\n",
    "  \"typing-extensions>=4.9\"\n",
    "\n",
    "# Document parsing stack\n",
    "%pip install -Uq \\\n",
    "  \"unstructured[all-docs]>=0.15.11,<0.16\" \\\n",
    "  \"pymupdf>=1.24.10\" \\\n",
    "  \"pdfplumber>=0.11.4\" \\\n",
    "  \"pillow>=10.4.0\" \\\n",
    "  \"pytesseract>=0.3.13\"\n",
    "\n",
    "# Windows MIME helper (no-op on Linux/macOS)\n",
    "%pip install -Uq \"python-magic-bin>=0.4.14\" || true\n",
    "\n",
    "print(\"✔ Dependencies installed. If your kernel had older packages loaded, restart the kernel once.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a776058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, base64, json, uuid, shutil, pathlib, tempfile\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# --- Gemini API key ---\n",
    "# Set the key in your environment as GEMINI_API_KEY or paste below (not recommended to hardcode).\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
    "assert GEMINI_API_KEY, \"Please set GEMINI_API_KEY in your environment (.env) before proceeding.\"\n",
    "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
    "\n",
    "# Silence LangChain's verbose tracing unless you want it\n",
    "#os.environ.setdefault(\"LANGCHAIN_TRACING_V2\", \"false\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc892b0b",
   "metadata": {},
   "source": [
    "## Models: Gemini 2.5 Pro (multimodal) + Gemini Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0065d0a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'langchain' has no attribute 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     18\u001b[39m embeddings = GoogleGenerativeAIEmbeddings(\n\u001b[32m     19\u001b[39m     model=EMBED_MODEL,\n\u001b[32m     20\u001b[39m     google_api_key=GEMINI_API_KEY,  \u001b[38;5;66;03m# <- forces API key auth (no ADC)\u001b[39;00m\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Multimodal LLM: Gemini 2.5 Pro (or use \"gemini-2.5-flash\" for lower latency)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Docs / model IDs: https://ai.google.dev/gemini-api/docs/models  :contentReference[oaicite:1]{index=1}\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Vertex model page (for reference): gemini-2.5-pro  :contentReference[oaicite:2]{index=2}\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m llm = \u001b[43mChatGoogleGenerativeAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-2.5-pro\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgoogle_api_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGEMINI_API_KEY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# <- forces API key auth (no ADC)\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✔ Gemini embeddings and LLM initialized.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Degree\\MachineLearning\\chat_rag\\.venv\\Lib\\site-packages\\langchain_core\\load\\serializable.py:115\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    114\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Degree\\MachineLearning\\chat_rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\base.py:108\u001b[39m, in \u001b[36m_get_verbosity\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_verbosity\u001b[39m() -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_verbose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Degree\\MachineLearning\\chat_rag\\.venv\\Lib\\site-packages\\langchain_core\\globals.py:86\u001b[39m, in \u001b[36mget_verbose\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     67\u001b[39m         warnings.filterwarnings(\n\u001b[32m     68\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     69\u001b[39m             message=(\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m             ),\n\u001b[32m     73\u001b[39m         )\n\u001b[32m     74\u001b[39m         \u001b[38;5;66;03m# N.B.: This is a workaround for an unfortunate quirk of Python's\u001b[39;00m\n\u001b[32m     75\u001b[39m         \u001b[38;5;66;03m#       module-level `__getattr__()` implementation:\u001b[39;00m\n\u001b[32m     76\u001b[39m         \u001b[38;5;66;03m# https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m         \u001b[38;5;66;03m# deprecation warnings directing them to use `set_verbose()` when they\u001b[39;00m\n\u001b[32m     85\u001b[39m         \u001b[38;5;66;03m# import `langchain.verbose`.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m         old_verbose = \u001b[43mlangchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     88\u001b[39m     old_verbose = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'langchain' has no attribute 'verbose'"
     ]
    }
   ],
   "source": [
    "# --- Environment & Keys ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # reads .env if present\n",
    "GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")\n",
    "assert GEMINI_API_KEY, \"Set GOOGLE_API_KEY (or GEMINI_API_KEY) in your environment or .env file.\"\n",
    "\n",
    "# Make sure the SDKs see the key (prevents DefaultCredentialsError / ADC fallback)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "\n",
    "# --- LangChain + Gemini models ---\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "# Embeddings: latest stable Gemini embedding model\n",
    "# Docs: https://ai.google.dev/gemini-api/docs/embeddings  :contentReference[oaicite:0]{index=0}\n",
    "EMBED_MODEL = \"gemini-embedding-001\"\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=EMBED_MODEL,\n",
    "    google_api_key=GEMINI_API_KEY,  # <- forces API key auth (no ADC)\n",
    ")\n",
    "\n",
    "# Multimodal LLM: Gemini 2.5 Pro (or use \"gemini-2.5-flash\" for lower latency)\n",
    "# Docs / model IDs: https://ai.google.dev/gemini-api/docs/models  :contentReference[oaicite:1]{index=1}\n",
    "# Vertex model page (for reference): gemini-2.5-pro  :contentReference[oaicite:2]{index=2}\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0.2,\n",
    "    google_api_key=GEMINI_API_KEY,  # <- forces API key auth (no ADC)\n",
    ")\n",
    "\n",
    "print(\"✔ Gemini embeddings and LLM initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f3a43d",
   "metadata": {},
   "source": [
    "## Document ingestion (PDF/DOCX/PPTX) with Unstructured + extras\n",
    "Extracts text, tables, and images; preserves page numbers and element types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035eb0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.partition.docx import partition_docx\n",
    "from unstructured.partition.pptx import partition_pptx\n",
    "import pdfplumber, fitz  # pymupdf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def load_file(path: str):\n",
    "    path = str(path)\n",
    "    suffix = pathlib.Path(path).suffix.lower()\n",
    "    if suffix == \".pdf\":\n",
    "        return partition_pdf(\n",
    "            filename=path,\n",
    "            extract_images_in_pdf=True,\n",
    "            infer_table_structure=True,\n",
    "            strategy=\"hi_res\",\n",
    "        )\n",
    "    if suffix == \".docx\":\n",
    "        return partition_docx(filename=path, include_page_breaks=True)\n",
    "    if suffix == \".pptx\":\n",
    "        return partition_pptx(filename=path, include_page_breaks=True)\n",
    "    raise ValueError(f\"Unsupported file type: {suffix}\")\n",
    "\n",
    "def pil_to_data_uri(img: Image.Image) -> str:\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format=\"PNG\")\n",
    "    b64 = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "    return f\"data:image/png;base64,{b64}\"\n",
    "\n",
    "def pdf_page_images(path: str, max_per_page: int = 8):\n",
    "    \"\"\"Extract raster images for each page (for multimodal grounding).\"\"\"\n",
    "    doc = fitz.open(path)\n",
    "    out = {}\n",
    "    for pno in range(len(doc)):\n",
    "        page = doc[pno]\n",
    "        images = []\n",
    "        for i, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            pix = fitz.Pixmap(doc, xref)\n",
    "            if pix.n >= 4:  # RGBA -> RGB\n",
    "                pix = fitz.Pixmap(fitz.csRGB, pix)\n",
    "            img_pil = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "            images.append(pil_to_data_uri(img_pil))\n",
    "            if len(images) >= max_per_page:\n",
    "                break\n",
    "        out[pno+1] = images\n",
    "    doc.close()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f5759",
   "metadata": {},
   "source": [
    "## Chunking and multi-vector indexing\n",
    "Splits text into chunks, keeps parent references, and generates auxiliary image/tables captions when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63386349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \", \", \" \"]\n",
    ")\n",
    "\n",
    "def element_to_text(e) -> str:\n",
    "    # Unstructured elements have .text and .category\n",
    "    txt = getattr(e, 'text', '') or ''\n",
    "    cat = getattr(e, 'category', '') or ''\n",
    "    return f\"[{cat}] {txt}\".strip()\n",
    "\n",
    "def build_parent_and_children(file_path: str) -> Dict[str, Any]:\n",
    "    elements = load_file(file_path)\n",
    "    raw_texts = [element_to_text(e) for e in elements if element_to_text(e)]\n",
    "    full_text = \"\\n\".join(raw_texts)\n",
    "\n",
    "    # Parent doc stores the full text (and metadata)\n",
    "    parent_id = str(uuid.uuid4())\n",
    "    parent_doc = Document(\n",
    "        page_content=full_text[:2_000_000],  # safety cap\n",
    "        metadata={\n",
    "            \"source\": str(file_path),\n",
    "            \"parent_id\": parent_id,\n",
    "            \"modality\": \"text+tables+images (extracted)\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Child chunks to be embedded\n",
    "    child_docs = []\n",
    "    for i, chunk in enumerate(text_splitter.split_text(full_text)):\n",
    "        child_docs.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\"parent_id\": parent_id, \"chunk\": i, \"source\": str(file_path)}\n",
    "        ))\n",
    "\n",
    "    # Also extract raster images per page (data URIs) for multimodal context-at-query time\n",
    "    images_by_page = {}\n",
    "    if str(file_path).lower().endswith(\".pdf\"):\n",
    "        images_by_page = pdf_page_images(file_path)\n",
    "\n",
    "    return {\"parent\": parent_doc, \"children\": child_docs, \"images_by_page\": images_by_page}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e010a",
   "metadata": {},
   "source": [
    "## Vector store and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9db8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# Persistent Chroma directory (optional)\n",
    "VEC_DIR = \"chroma_gemini_rag\"\n",
    "\n",
    "# Child store: vector index of chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"mm_rag_chunks\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=VEC_DIR,\n",
    ")\n",
    "\n",
    "# Parent store: in-memory docstore keyed by parent_id\n",
    "docstore = InMemoryStore()\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    id_key=\"parent_id\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e953c",
   "metadata": {},
   "source": [
    "## Index documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_files(files: List[str]):\n",
    "    for fp in files:\n",
    "        bundle = build_parent_and_children(fp)\n",
    "        parent, children = bundle[\"parent\"], bundle[\"children\"]\n",
    "        parent_id = parent.metadata[\"parent_id\"]\n",
    "        # 1) Store the parent\n",
    "        docstore.mset([(parent_id, parent)])\n",
    "        # 2) Add children to vector DB\n",
    "        vectorstore.add_documents(children)\n",
    "    print(f\"Indexed {len(files)} file(s).\")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7fd9c",
   "metadata": {},
   "source": [
    "## Answer generation (multimodal)\n",
    "At query time, we retrieve top chunks and attach matching page images (if any) to the Gemini 2.5 Pro prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cda00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "SYSTEM_PROMPT = (    \"You are a precise assistant. Answer strictly from the provided context. \"\n",
    "    \"Cite page numbers or slide numbers when possible. If unsure, say you don't know.\")\n",
    "\n",
    "PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"human\", \"Query: {query}\\n\\nContext:\\n{context}\\n\\nAnswer:\"),\n",
    "])\n",
    "\n",
    "def docs_to_bullets(docs: List[Document]) -> str:\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        src = d.metadata.get(\"source\", \"?\")\n",
    "        chunk = d.metadata.get(\"chunk\", \"?\")\n",
    "        out.append(f\"- ({src} · chunk {chunk}) {d.page_content[:500]}\".strip())\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def pick_images_for_docs(docs: List[Document], images_by_file: Dict[str, Dict[int, List[str]]], max_images=4) -> List[str]:\n",
    "    imgs = []\n",
    "    for d in docs:\n",
    "        src = d.metadata.get(\"source\")\n",
    "        if not src:\n",
    "            continue\n",
    "        if src.lower().endswith(\".pdf\") and src in images_by_file:\n",
    "            # Take first page images we have (heuristic)\n",
    "            for page_no, uris in images_by_file[src].items():\n",
    "                for u in uris:\n",
    "                    if len(imgs) < max_images:\n",
    "                        imgs.append(u)\n",
    "                if len(imgs) >= max_images:\n",
    "                    break\n",
    "        if len(imgs) >= max_images:\n",
    "            break\n",
    "    return imgs\n",
    "\n",
    "def query_rag(query: str, k: int = 6):\n",
    "    # Retrieve top-k text chunks\n",
    "    chunks = retriever.vectorstore.similarity_search(query, k=k)\n",
    "    context = docs_to_bullets(chunks)\n",
    "\n",
    "    # Collect images related to these docs (if PDF sources were indexed)\n",
    "    # We maintain a map file->page->images during build; reconstruct it:\n",
    "    # For simplicity, we re-extract here for the top sources (cheap for small docs).\n",
    "    images_map = {}\n",
    "    for d in chunks:\n",
    "        src = d.metadata.get(\"source\",\"\")\n",
    "        if src.lower().endswith(\".pdf\") and src not in images_map:\n",
    "            try:\n",
    "                images_map[src] = pdf_page_images(src)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    images = pick_images_for_docs(chunks, images_map)\n",
    "\n",
    "    # Compose messages (attach up to 4 images as data URIs)\n",
    "    messages = [SystemMessage(content=SYSTEM_PROMPT)]\n",
    "    if images:\n",
    "        # HumanMessage can take a list of content parts: text + images\n",
    "        parts = [{\"type\":\"text\", \"text\": PROMPT.format(query=query, context=context).to_string()}]\n",
    "        for u in images:\n",
    "            parts.append({\"type\": \"image_url\", \"image_url\": u})\n",
    "        messages.append(HumanMessage(content=parts))\n",
    "    else:\n",
    "        messages.append(HumanMessage(content=PROMPT.format(query=query, context=context).to_string()))\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content, chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15d5919",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "# files_to_index = [\"/path/to/your.pdf\", \"/path/to/your.docx\", \"/path/to/slides.pptx\"]\n",
    "# index_files(files_to_index)\n",
    "\n",
    "# Then ask:\n",
    "# answer, supporting_chunks = query_rag(\"What are the key findings? Provide citations.\")\n",
    "# print(answer)\n",
    "# supporting_chunks[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb25b7",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Embeddings model: `gemini-embedding-001` (flexible output dims; default via LangChain).  \n",
    "- Multimodal LLM: `gemini-2.5-pro` (supports text, images, video, audio, and PDFs; long context).  \n",
    "- Vector DB: Chroma. Parent documents are kept in an in-memory docstore; retrieved via the multi-vector pattern.\n",
    "- You can switch to `gemini-2.5-flash` for lower cost/latency without changing the code.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
